\documentclass[11pt,a4paper]{article}
%Lorem Ipsum Einbindung
\usepackage{lipsum}

%Zeilenabstand
\linespread{1.5}
%graphics
\usepackage{graphicx}
\usepackage[margin=01in,includefoot]{geometry}
\usepackage{}
\usepackage{fancyhdr}
\usepackage{float}
%Einbindung Titelblatt mit PDF - Paket
\usepackage{pdfpages}

%Kopf, Fusszeilen
\pagestyle{fancy}
\fancyfoot{}
\fancyhead{}
\fancyfoot[R]{\thepage\ }
\usepackage{geometry}
\geometry{tmargin=25mm,bmargin=20mm,lmargin=27mm,rmargin=25mm}

%Literaturverzeichnis und Zitieren
\usepackage{apacite}
\bibliographystyle{apacite}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{nameref}

%URL nicht in anderer Schrift (z.B. Bibliografie)
\usepackage{url}
%Fussnoten am Ende der Seite
\usepackage[bottom]{footmisc}

\usepackage{hyperref}
\usepackage{graphicx} %Package für Grafiken
\usepackage{subfig} % Package für mehrere Grafiken nebeneinander
%\DeclareMathOperator{\atantwo}{atan2}

\begin{document}

%Titelseite Einbindung PDF
\begin {titlepage}
%\includepdf{Titelblatt}
\end {titlepage}
\newpage


\section*{Abstract} \label{sec:Abstract}

In dieser Arbeit werden die Notwendigkeit, wie auch Gegenargumente einer globalen Finanzmarktregulierung analysiert und diskutiert. Die nachfolgende Argumentation st\"utzt sich auf ein intensives Literaturstudium. Aus der Untersuchung wird ersichtlich, dass trotz zus\"atzlicher Kosten, einer steigenden Sicherheit und der Gefahr von regulatorischer Arbitrage eine globale Finanzmarktregulierung einen positiven Einfluss auf die Weltwirtschaft haben k\"onnte. Denn falsche Anreize und starke negative Externalit\"aten, wie explizite und implizite Staatsgarantien, aber auch die Gefahr einer Kreditklemme, verzerren den Markt und f\"uhren schliesslich zu weniger Wohlstand.

\newpage
%Inhaltsverzeichnis
\tableofcontents
\newpage
%Tabellen- und Abbildungsverzeichnis, plus hinzuf\"ugen zu TOC, ohne Nummerierung
\listoffigures
\addcontentsline{toc}{section}{\listfigurename}
\vspace{12cm}
Aufgrund der besseren Lesbarkeit und der Einfachheit halber wird in der vorliegenden Arbeit nur die männliche Form verwendet. Die weibliche Form ist selbstverständlich immer miteingeschlossen.

\newpage
\section{Einleitung}\label{sec:Einleitung}
\begin{quotation}
``Regulation is necessary, particularly in a sector, like the banking sector, which exposes countries and people to a risk'' \cite{lagarde2018}.
\end{quotation}

\section{Data Cleaning and Preparation}

\section{New York Inspection Data}

The main dataset for our project is retrieved from the \href{https://data.ny.gov/Economic-Development/Retail-Food-Store-Inspections-Current-Ratings/d6dy-3h7r}{official site of the State of New York}. It contains a total of 28'300 A to C ratings from of food store inspections. When we downloaded the data on the XXX it has been the version of June 26, 2019 with observations from March 2018 to March 2019.\\
\\
The data published by the state of New York includes not only a list of all the food ratings of different shops but also part of the history of the inspection grade development. This means the same shops could appear several times with different issues. To avoid the problem of having the identical shop more than one time we ordered the dataset ascending to the inspection date. Considering only the newest entry of every shop would have led to a bias regarding the hygiene grade because the bad graded shops improved themselves. Thus we decided to only take in account the oldest entry of every shop. This resulted in a data frame of around 7500 unique shops in the city of New York.
\\
Additionally, the data set reveals not only the shops trade name but also its owner. This enabled us to identify if the shop is part of a chain. We added this parameter beside to the number of shops which belong to the specific chain. It seems possible to have an impact on the hygiene grade if the shop is part of a bigger chain. 
\\
We will predict the food store inspection ratings from this dataset with different covariates whose derivation is outlined in the following sections.

\section{Chain Information}

\section{Spatial Data}

Some of the predictors rely on geolocation data. The cleaned inspection dataset contain spatial information in form of latitude and longitude data for most of the observations. The latitude and longitude information, however, is embedded in a larger address string. We create a function to extract the location data in two new columns. A check for \texttt{NA} values reveals that 748 values missing location information. The exact address is available for all shops though. Therefore, we combine, street, city and ZIP code to a single string that can be used with the Google Maps API\footnote{In essence, the Google Maps API is a free service offered by Google. It requires only a one-time registration with a valid email address. Afterwards, it generates an API Key that must be included in the R-script  with the command \texttt{register\_google(key = "API KEY")}.} to obtain the missing latitude and longitude details. A Map of the completed data is illustrated in Figure~\ref{map1} on state and city level.\\
\begin{figure}
\captionof{figure}{Geographic Distribution of the inspected Shops \label{map1}} 
\subfloat[New York State]{%
       \includegraphics[width=0.5\textwidth]{Plots/Plot1_Map.png}
     }
     \hfill
     \subfloat[New York City]{%
       \includegraphics[width=0.5\textwidth]{Plots/Plot2_Map.png}
     }
\end{figure}
With complete spatial information, we now compute the two variables "shops density in 1km radius" as well as "rating of the closest neighbor". To get the distances of coordinates in meters, we apply the Haversine Formula\footnote{The Haversine Formula has its roots in spherical trigonometry and calculates the geodesic distance -- the shortest path between two points on a curved surface of a sphere, like the Earth. It needs to be mentioned that Haversine Formula does not take into account changes in altitude. However, it provides sufficient accuracy for the scope of this project.}
\begin{equation} a = \sin^{2}(\frac{\Delta\alpha}{2}) + \cos(\alpha_{1})\cos(\alpha_{2})\sin^{2}(\frac{\Delta\lambda}{2})  \end{equation}
\begin{equation} d = R \left( 2 \atantwo (\sqrt{a}, \sqrt{1-a} )  \right) \end{equation}
where $\alpha_{i}$ are latitudes, $\lambda_{i}$ are longitudes, $R$ is the earth's radius ($6371\times10^{3}m$).

\section{Subway Data}

In a next step, we amend our New York City inspection data with location information of subway stations. We use again \href{https://data.ny.gov/Transportation/NYC-Transit-Subway-Entrance-And-Exit-Data/i9wp-a4ja}{an official dataset provided by the State of New York}. The dataset contains the location of every subway entrance and its corresponding station. We are only interested in the station. Therefore, we remove all duplicates of stations with multiple entrances and then use again the Haversine Formula to find the distance of the closest station to every shop.

\subsection{Google web scraper}

While doing research to find datasets with regards to food inspections we decided to gather more information from webpages. The reviews from customers, written on google places, could give us information about how individual persons have experienced their visit to the specific food shop. In order to gather this information from the internet we set up a web scraper. We used the package \textit{RSelenium} to execute an automated google search for all the food shops. This included a lot of trial and error due to unforeseen changes of the xPaths and other errors. A very fast but costly alternative would have been to use the Google Places API which provides access to all information stored by Google Places. Although it takes much more time we decided to go ahead with scraping the data ourselves. In terms of time a lot of patience was needed. Most of the errors only showed up in the middle of the scraping process, which had a duration of about 24 hours for all the food shops. Unfortunately most of the errors occurred towards the end of the scraping. Therefore, it was only possible to adjust the function after the script run during the night. This resulted in a lot of waiting and adjusting but finally the data was scraped successfully. Nonetheless the scraped data was not in a useable format yet. After tidying and joining the data to the original dataset it was ready to be used. \\

A brief analysis of the dataset revealed the incompleteness of the ratings. Approximately 40\% of the shops don't have an entry in google places and thus neither rating. After a lot of discussion on how to handle the problem we agreed on using the number of reviews as a parameter of the internet popularity. 



\subsection{Airbnb data and spatial analysis}

In order to acquire additional data and parameters on the location of the shops, we integrated data from Airbnb (average price and number of rooms). To join the original and the airbnb data frames a matching key is needed. Regarding the dataset only longitude and latitude were available. Therefore we used the above mentioned spatial data to assign the respective parameter to the ZIP codes in New York. Thereafter we grouped the data by ZIP code, calculated the respective means and counts and added it to the new tibble.
\section{Zusammenfassung und Diskussion der Resultate} \label{sec:Diskussion}

In dieser Arbeit wurden die Notwendigkeit und allf\"allige Gegenargumente einer globalen Finanzmarktregulierung untersucht. 
%Literaturverzeichnis
\urlstyle{rm}

\newpage
\bibliography{bib}
\newpage

\textbf{Eigenst\"andigkeitserkl\"arung und Zeichenzahl}\\
\\
«Ich erkl\"are hiermit, 
\begin{itemize}
\item[- ]dass ich die vorliegende Arbeit selbstständig, ohne fremde Hilfe (Lektorat, Übersetzungsdienstleister etc.) und ohne Verwendung anderer als der angegeben Hilfsmittel verfasst habe;
\item[- ]dass ich sämtliche verwendeten Quellen erwähnt und gemäss gängigen wissenschaftlichen Zitierregeln korrekt zitiert habe;
\item[- ]dass das Thema, die Arbeit oder Teile davon nicht bereits Gegenstand eines Leistungsnachweises einer anderen Veranstaltung oder Kurses waren;
\item[- ]dass ich ohne schriftliche Zustimmung der Universität keine Kopien dieser Arbeit an Dritte
aushändigen oder veröffentlichen werde, wenn ein direkter Bezug zur Universität St.Gallen
oder ihrer Dozierenden hergestellt werden kann;
\item[- ]dass ich mir bewusst bin, dass meine Arbeit elektronisch auf Plagiate überprüft werden kann
und ich hiermit der Universität St.Gallen laut Prüfungsordnung das Urheberrecht soweit einräume, wie es für die Verwaltungshandlungen notwendig ist;
\item[- ]dass ich mir bewusst bin, dass die Universität einen Verstoss gegen diese Eigenständigkeitserklärung sowie insbesondere die Inanspruchnahme eines Ghostwriter-Service verfolgt und dass daraus disziplinarische wie auch strafrechtliche Folgen resultieren können, welche zum Ausschluss von der Universität resp. zu einer späteren Titelaberkennung führen können»
\end{itemize}

(Zeichen: 44'978)
\end{document}